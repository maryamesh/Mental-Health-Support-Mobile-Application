{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell imports all the necessary libraries for model training, evaluation, and data processing, including tools for natural language processing (NLP) from the Hugging Face Transformers library, data handling with pandas, and metrics calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for handling model training, evaluation, and data processing.\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell loads the pre-trained RoBERTa model and its corresponding tokenizer, specifically fine-tuned for emotion classification. Additionally, it loads the SetFit/emotion dataset from Hugging Face for training and evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer for emotion classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", num_labels=28)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Load the SetFit/emotion dataset for training and evaluation\n",
    "dataset = load_dataset(\"SetFit/emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell defines a preprocessing function that tokenizes the text data and converts labels into a multi-label format suitable for emotion classification. The function is applied to the entire dataset, and the data is formatted for PyTorch compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessing function to tokenize the text and encode labels into a multi-label format\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    num_labels = 28\n",
    "    labels = np.zeros((len(examples['label']), num_labels))\n",
    "    for idx, label in enumerate(examples['label']):\n",
    "        labels[idx, label] = 1\n",
    "    inputs[\"labels\"] = torch.tensor(labels, dtype=torch.float32)\n",
    "    return inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell creates smaller subsets of the tokenized dataset for training and validation purposes by randomly selecting a specified number of examples from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smaller subsets for training and validation\n",
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_validation_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell defines the training arguments, such as batch size, number of epochs, and logging settings, to fine-tune the model on a CPU. It initializes the Trainer object with the specified arguments and starts the fine-tuning process on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments for model fine-tuning on CPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for model fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start fine-tuning the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell saves the fine-tuned model and tokenizer to a local directory for future use or further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer to a specified directory\n",
    "model.save_pretrained(\"./fine_tuned_model_epoch_1\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model_epoch_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell reloads the fine-tuned model and tokenizer, reinitializes the Trainer object for evaluation, and evaluates the model on the validation dataset to check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the fine-tuned model and tokenizer for evaluation\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./fine_tuned_model_epoch_1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model_epoch_1\")\n",
    "\n",
    "# Re-initialize the Trainer for model evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=small_validation_dataset,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell loads the accuracy metric, computes predictions from the model, and evaluates its accuracy on the validation dataset, providing a quantitative measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accuracy metric to evaluate the model performance\n",
    "metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "\n",
    "# Compute the predictions and evaluate accuracy\n",
    "encoded_inputs = tokenizer(small_validation_dataset[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "labels = torch.tensor(small_validation_dataset[\"label\"])\n",
    "results = metric.compute(predictions=predictions, references=labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell loads a custom dataset, preprocesses it by one-hot encoding the sentiment labels and tokenizing the text, and converts the processed data into a PyTorch-compatible dataset for further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a custom dataset and preprocess it for fine-tuning\n",
    "file_path = \"F:/metal health app/mood tracking/tweet_emotions.csv\"\n",
    "new_dataset = pd.read_csv(file_path)\n",
    "\n",
    "# One-hot encode labels using LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(new_dataset['sentiment'])\n",
    "\n",
    "# Adjust labels to match the number of target labels\n",
    "num_labels = 28\n",
    "adjusted_labels = np.zeros((len(labels), num_labels))\n",
    "for i, label in enumerate(labels):\n",
    "    adjusted_labels[i, :len(label)] = label\n",
    "\n",
    "# Tokenize the custom dataset\n",
    "encoded_inputs = tokenizer(list(new_dataset['content']), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "encoded_inputs[\"labels\"] = torch.tensor(adjusted_labels, dtype=torch.float32)\n",
    "\n",
    "# Convert the preprocessed data into a PyTorch dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "custom_dataset = CustomDataset(encoded_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### This cell sets up the training arguments and initializes the Trainer for fine-tuning the model on the custom dataset. It then starts the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments for fine-tuning the model on the custom dataset\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_custom\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for fine-tuning on the custom dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=custom_dataset,\n",
    "    eval_dataset=custom_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start fine-tuning the model on the custom dataset\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell reloads the fine-tuned model from a checkpoint, sets it to evaluation mode, and uses the Trainer object to evaluate the model's performance on the custom dataset. The predictions are then converted to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model from the checkpoint for evaluation on the custom dataset\n",
    "checkpoint_path = r\"F:\\\\metal health app\\\\mood tracking\\\\results_custom\\\\checkpoint-11000\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "# Define the Trainer for model evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=custom_dataset\n",
    ")\n",
    "\n",
    "# Evaluate the model on the custom dataset\n",
    "results = trainer.predict(custom_dataset)\n",
    "print(\"Predictions:\", results.predictions)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = torch.sigmoid(torch.tensor(results.predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell provides custom input text and generates predictions for each emotion label using the fine-tuned model. It displays the probability of each emotion and makes binary predictions based on a specified threshold to identify the most likely emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion labels and provide custom input text for emotion prediction\n",
    "emotion_labels = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\",\n",
    "    \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\",\n",
    "    \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\",\n",
    "    \"nervousness\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\",\n",
    "    \"sadness\", \"surprise\", \"neutral\"\n",
    "]\n",
    "\n",
    "custom_input = [\"I am feeling wonderful today!\", \"I am so annoyed and angry right now!\", \"I'm confused about what to do next.\"]\n",
    "\n",
    "# Tokenize custom input texts\n",
    "encoded_inputs = tokenizer(custom_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Generate predictions using the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = torch.sigmoid(outputs.logits)\n",
    "\n",
    "# Display probabilities for each emotion label for each input\n",
    "print(\"\\nProbabilities for each emotion label for each input:\")\n",
    "for i, text in enumerate(custom_input):\n",
    "    print(f\"\\nText: {text}\")\n",
    "    for emotion, prob in zip(emotion_labels, probabilities[i].tolist()):\n",
    "        print(f\"  {emotion}: {prob:.4f}\")\n",
    "\n",
    "# Adjust threshold and make binary predictions\n",
    "threshold = 0.5\n",
    "predictions = (probabilities > threshold).int()\n",
    "\n",
    "# Display predicted emotions based on threshold\n",
    "print(\"\\nPredicted emotions based on adjusted threshold:\")\n",
    "for i, text in enumerate(custom_input):\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Predicted Emotions: \", end=\"\")\n",
    "    for emotion, pred in zip(emotion_labels, predictions[i].tolist()):\n",
    "        if pred == 1:\n",
    "            print(f\"{emotion}\", end=\", \")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
